<!doctype html>
<html lang="en">
<head>
<title>Image classification in the Medical Domain</title>
<meta property="og:title" content=Your Project Name" />
<meta name="twitter:title" content="Your Project Name" />
<meta name="description" content="Your project about your cool topic described right here." />
<meta property="og:description" content="Your project about your cool topic described right here." />
<meta name="twitter:description" content="Your project about your cool topic described right here." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" />
<!-- bootstrap for mobile-friendly layout -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">

</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">Image Classification for CT Scans</nobr>
 <nobr class="widenobr">For CS 7150</nobr>
 </h1>
 </div>
</div><!-- end nd-pageheader -->

<nav class="navbar navbar-expand-lg navbar-light bg-light">
  <div class="collapse navbar-collapse" id="navbarNavAltMarkup">
    <div class="navbar-nav">
      <a class="nav-link" href="index.html">Home</a>
      <a class="nav-link active" href="proposal.html">Proposal</a>
    </div>
  </div>
</nav>

<div class="container">
<div class="row">
<div class="col justify-content-center text-center">
<h2>Image classification in the Medical Domain</h2>
<p>Top image classification models work well on the major datasets, but how do they perform on CT scans?</p>
</div>
</div>
<div class="row">
<div class="col">

<h2 name="title">Project Proposal</h2>

<p> A common benchmark for image classification is <a href="https://www.image-net.org/">ImageNet</a>. The dataset tests the model’s ability to generalize prediction across many image categories. Although this benchmark is useful in providing a common objective for all image classification model, it does not determine the efficacy of the model in more specific and specialized domains. In this project, we will look to understand the performance of top models in the medical imaging domain. The dataset that we use is from a current  <a href="https://www.kaggle.com/competitions/rsna-2022-cervical-spine-fracture-detection"> Kaggle competition </a>. The dataset is comprised of Cervical Spine CT scans taken from 12 sites around the world by the Radiological Society of North America. The objective is to detect fractures in the spine scans. These images present a different problem as the task of classification is tough for highly trained humans let alone machine learning models.
</p>

<p> 
  We will be comparing the performance of 2 image classification models that use different architectures to understand how well they can handle the CT scan classification task. The first model that we will be evaluating is the Vision Transformer. This model utilizes the Transformer architecture for classification rather than the widely accepted CNN archicture. The transformer architecture’s success in the NLP field made it a strong candidate for use in the computer vision field. The second model that we will evaluate is <a href="https://arxiv.org/abs/1512.03385">ResNet [2]</a>. This represents one of the best performing CNN architecture models. Our hope is to understand whether these 2 models, which represent some of the best models that are currently available, are suitable to specialized tasks in medicine. 
</p>

<h2> Literature Review </h2>

<p> The paper we are primarily citing, <a href="https://arxiv.org/abs/2010.11929">An Image is Worth 16x16 Words: Transformers For Image Recognition At Scale [1]</a>, defends the argument that the reliance on current state of the art CNNs is not necessary and a pure transformer applied directly to sequence of image patches can perform very well on image classification tasks. The authors conclude that Vision Transformers can achieve “excellent” results while requiring “substantially” fewer computational resources to train compared to convolutional networks. </p>

<p>As stated by the paper, the idea of self-attention-based architecture and specifically Transformers were proposed by Vaswani et al. in their paper <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need in 2017[3] </a>. Their focus was on machine translation tasks and their model achieved 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. However, Dosovitskiy et al. acknowledge that convolutional networks remain dominant in the computer vision field. Other research has applied self-attention architecture in local neighborhoods for each query pixel instead of globally as done by Parmar et al. in their paper <a href="https://arxiv.org/abs/1802.05751">Image Transformer [4]</a>.</p>

<p>In this study, the authors hope to scale the self-attention architecture through Transformers on large datasets, that have previously only been by CNNs as done by Djolonga et al. in their paper <a href="https://arxiv.org/abs/2007.08558"> On robustness and transferability of convolutional neural networks [5] </a> in 2020. This paper expands on that research by continuing to focus on the aforementioned datasets, but train Transformers instead of ResNet-based models. </p>
</p>

<h3>References</h3>


<p>[1] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T. & Houlsby, N. "An image is worth 16x16 words: Transformers for image recognition at scale". <em>arXiv preprint arXiv:2010.11929,</em>  2020.</p>

<p>[2] He, K., Zhang, X., Ren, S., and Sun, J., “Deep Residual Learning for Image Recognition”, <em> arXiv preprints arXiv:1512.03385,</em> 2015.
</p>

<p>[3] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. "Attention is all you need." <em>In NIPS</em>, 2017.</p>

<p>[4] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. "Image Transformer". <em>In ICML</em>, 2018.</p>

<p>[5] Josip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D’Amour, Dan Moldovan, Sylvan Gelly, Neil Houlsby, Xiaohua Zhai, and Mario Lucic. "On robustness and transferability of convolutional neural networks". <em>arXiv:2007.08558</em>, 2020.</p>

<h2>Team Members</h2>
                                                   
<p>Aniket Lachyankar (lachyankar.a@northeastern.edu) and Satwik Kamarthi (kamarthi.s@northeastern.edu) </p>

  
</div><!--col-->
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://cs7150.baulab.info/">About CS 7150</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
// Google analytics below.
window.dataLayer = window.dataLayer || [];
</script>
</html>
