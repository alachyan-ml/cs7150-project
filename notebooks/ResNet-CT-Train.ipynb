{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38d59266-91c5-480e-b709-9d80cc8dfc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q transformers\n",
    "#!pip install -U albumentations\n",
    "# !pip install torchsummary\n",
    "# !pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6fa2d8b-5bdb-42e0-9bee-20f3035e4c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader,random_split,Subset\n",
    "from transformers import AutoFeatureExtractor, ResNetForImageClassification\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import albumentations as A\n",
    "import albumentations.pytorch as ATorch\n",
    "from albumentations.augmentations.geometric.transforms import PadIfNeeded\n",
    "from albumentations.augmentations.transforms import Normalize\n",
    "from albumentations.augmentations.geometric.resize import Res\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import gc\n",
    "import random\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "545aba18-9e29-48a8-87e3-3b4e02240363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(f'Selected device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a73afdc-30e7-406f-b127-7ab250323aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16952 Samples\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"project_data/train.csv\")\n",
    "\n",
    "def get_frac_data_from_patient(target):\n",
    "    patient_index = list(image_data.class_to_idx.values()).index(target)\n",
    "    patient_name = list(image_data.class_to_idx.keys())[patient_index]\n",
    "    row_for_pt = train_df[train_df[\"StudyInstanceUID\"] == patient_name]\n",
    "    return row_for_pt.iloc[:,1:].values\n",
    "\n",
    "\n",
    "path = \"project_data/project_analysis/sag\"\n",
    "image_data = ImageFolder(root=path, target_transform= get_frac_data_from_patient)\n",
    "print(f\"{len(image_data)} Samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bb97451-2ceb-4af6-8a7a-de5101d41c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_shapes(dataset):\n",
    "    tensor_t = ToTensor()\n",
    "    shapes = []\n",
    "    for sample in dataset:\n",
    "        shapes.append(sample[0].size)\n",
    "    return torch.Tensor(shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ad483d6-6009-4012-aa6b-12ac5b993f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_sequence(dataset):\n",
    "    i = 0\n",
    "    img_data_list = []\n",
    "    for sample in random.sample(range(len(dataset)), len(dataset) // 4):\n",
    "        # print(sample[0])\n",
    "        img = dataset[sample]\n",
    "        img_data = img[0].getdata()\n",
    "        img_data_list.append(img_data)\n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "    return np.concatenate(img_data_list, axis=0) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f41e506-5fc6-4256-a707-88f295702048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Shapes\n",
      "Max Height: 768.0, Max Width: 1082.0\n",
      "Getting mean and std\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "stat_file = 'image_stats.pickle'\n",
    "\n",
    "if not os.path.exists(stat_file):\n",
    "    print(\"Getting Shapes\")\n",
    "    shapes = get_image_shapes(image_data)\n",
    "    print(f\"Max Height: {torch.max(shapes[:,0])}, Max Width: {torch.max(shapes[:,1])}\")\n",
    "    print(\"Getting mean and std\")\n",
    "    data = get_image_sequence(image_data)\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std = np.std(data, axis=0)\n",
    "    print(f\"mean: {mean}, std:{std}\")\n",
    "    dataset_stats = {\"mean\": mean, \"std\": std, \"shapes\": shapes}\n",
    "    with open(stat_file, 'wb') as f:\n",
    "        pickle.dump(dataset_stats, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open(stat_file, 'rb') as handle:\n",
    "        dataset_stats = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1bcf9a-7175-4544-ae63-b5502a1e9179",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_stats['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4437033-fb31-43b6-8eb7-0dfecc10ff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dataset, transform=None):\n",
    "        self.image_data = image_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.image_data[idx]\n",
    "        image = np.array(image)\n",
    "        label = label.astype(np.float32)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "        return image, np.squeeze(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf14f2f4-1d30-485c-9a84-d96a5d26ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_augmentations = A.Compose([\n",
    "    PadIfNeeded(min_height=int(torch.max(dataset_stats[\"shapes\"][:,0]).item()), min_width=int(torch.max(dataset_stats[\"shapes\"][:,1]).item())),\n",
    "    A.Normalize(mean=dataset_stats[\"mean\"].tolist(), std=dataset_stats[\"std\"].tolist(), max_pixel_value=1),\n",
    "    ATorch.transforms.ToTensorV2(),\n",
    "])\n",
    "    \n",
    "image_dataset = ImageDataset(image_data, base_augmentations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dca3a7c7-f2e6-4490-8cac-5ed1171b03fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-50 and are newly initialized because the shapes did not match:\n",
      "- classifier.1.weight: found shape torch.Size([1000, 2048]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNetForMultiClassPrediction(\n",
       "  (resnet): ResNetForImageClassification(\n",
       "    (resnet): ResNetModel(\n",
       "      (embedder): ResNetEmbeddings(\n",
       "        (embedder): ResNetConvLayer(\n",
       "          (convolution): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "          (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (pooler): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (encoder): ResNetEncoder(\n",
       "        (stages): ModuleList(\n",
       "          (0): ResNetStage(\n",
       "            (layers): Sequential(\n",
       "              (0): ResNetBottleNeckLayer(\n",
       "                (shortcut): ResNetShortCut(\n",
       "                  (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "                (layer): Sequential(\n",
       "                  (0): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (1): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (2): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): Identity()\n",
       "                  )\n",
       "                )\n",
       "                (activation): ReLU()\n",
       "              )\n",
       "              (1): ResNetBottleNeckLayer(\n",
       "                (shortcut): Identity()\n",
       "                (layer): Sequential(\n",
       "                  (0): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (1): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (2): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): Identity()\n",
       "                  )\n",
       "                )\n",
       "                (activation): ReLU()\n",
       "              )\n",
       "              (2): ResNetBottleNeckLayer(\n",
       "                (shortcut): Identity()\n",
       "                (layer): Sequential(\n",
       "                  (0): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (1): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (2): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): Identity()\n",
       "                  )\n",
       "                )\n",
       "                (activation): ReLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): ResNetStage(\n",
       "            (layers): Sequential(\n",
       "              (0): ResNetBottleNeckLayer(\n",
       "                (shortcut): ResNetShortCut(\n",
       "                  (convolution): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "                (layer): Sequential(\n",
       "                  (0): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (1): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (2): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): Identity()\n",
       "                  )\n",
       "                )\n",
       "                (activation): ReLU()\n",
       "              )\n",
       "              (1): ResNetBottleNeckLayer(\n",
       "                (shortcut): Identity()\n",
       "                (layer): Sequential(\n",
       "                  (0): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (1): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (2): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): Identity()\n",
       "                  )\n",
       "                )\n",
       "                (activation): ReLU()\n",
       "              )\n",
       "              (2): ResNetBottleNeckLayer(\n",
       "                (shortcut): Identity()\n",
       "                (layer): Sequential(\n",
       "                  (0): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (1): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (2): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): Identity()\n",
       "                  )\n",
       "                )\n",
       "                (activation): ReLU()\n",
       "              )\n",
       "              (3): ResNetBottleNeckLayer(\n",
       "                (shortcut): Identity()\n",
       "                (layer): Sequential(\n",
       "                  (0): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (1): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (2): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): Identity()\n",
       "                  )\n",
       "                )\n",
       "                (activation): ReLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): ResNetStage(\n",
       "            (layers): Sequential(\n",
       "              (0): ResNetBottleNeckLayer(\n",
       "                (shortcut): ResNetShortCut(\n",
       "                  (convolution): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                  (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "                (layer): Sequential(\n",
       "                  (0): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (1): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (2): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): Identity()\n",
       "                  )\n",
       "                )\n",
       "                (activation): ReLU()\n",
       "              )\n",
       "              (1): ResNetBottleNeckLayer(\n",
       "                (shortcut): Identity()\n",
       "                (layer): Sequential(\n",
       "                  (0): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (1): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (2): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): Identity()\n",
       "                  )\n",
       "                )\n",
       "                (activation): ReLU()\n",
       "              )\n",
       "              (2): ResNetBottleNeckLayer(\n",
       "                (shortcut): Identity()\n",
       "                (layer): Sequential(\n",
       "                  (0): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (1): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (2): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): Identity()\n",
       "                  )\n",
       "                )\n",
       "                (activation): ReLU()\n",
       "              )\n",
       "              (3): ResNetBottleNeckLayer(\n",
       "                (shortcut): Identity()\n",
       "                (layer): Sequential(\n",
       "                  (0): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (1): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (2): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): Identity()\n",
       "                  )\n",
       "                )\n",
       "                (activation): ReLU()\n",
       "              )\n",
       "              (4): ResNetBottleNeckLayer(\n",
       "                (shortcut): Identity()\n",
       "                (layer): Sequential(\n",
       "                  (0): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (1): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (2): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): Identity()\n",
       "                  )\n",
       "                )\n",
       "                (activation): ReLU()\n",
       "              )\n",
       "              (5): ResNetBottleNeckLayer(\n",
       "                (shortcut): Identity()\n",
       "                (layer): Sequential(\n",
       "                  (0): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (1): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (2): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): Identity()\n",
       "                  )\n",
       "                )\n",
       "                (activation): ReLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): ResNetStage(\n",
       "            (layers): Sequential(\n",
       "              (0): ResNetBottleNeckLayer(\n",
       "                (shortcut): ResNetShortCut(\n",
       "                  (convolution): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                  (normalization): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "                (layer): Sequential(\n",
       "                  (0): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (1): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (2): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): Identity()\n",
       "                  )\n",
       "                )\n",
       "                (activation): ReLU()\n",
       "              )\n",
       "              (1): ResNetBottleNeckLayer(\n",
       "                (shortcut): Identity()\n",
       "                (layer): Sequential(\n",
       "                  (0): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (1): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (2): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): Identity()\n",
       "                  )\n",
       "                )\n",
       "                (activation): ReLU()\n",
       "              )\n",
       "              (2): ResNetBottleNeckLayer(\n",
       "                (shortcut): Identity()\n",
       "                (layer): Sequential(\n",
       "                  (0): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (1): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (2): ResNetConvLayer(\n",
       "                    (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activation): Identity()\n",
       "                  )\n",
       "                )\n",
       "                (activation): ReLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    )\n",
       "    (classifier): Sequential(\n",
       "      (0): Flatten(start_dim=1, end_dim=-1)\n",
       "      (1): Linear(in_features=2048, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Model Declaration\n",
    "\n",
    "class ResNetForMultiClassPrediction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNetForMultiClassPrediction, self).__init__()\n",
    "        self.resnet = ResNetForImageClassification.from_pretrained(\"microsoft/resnet-50\", num_labels=8, ignore_mismatched_sizes=True)\n",
    "        # Add Sigmoid for output between 0 and 1\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x).logits\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = ResNetForMultiClassPrediction()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b4e11a6-da0e-4989-9c1b-b6b50cf34678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Test functions (Taken from HW3 of CS7150 HWs)\n",
    "\n",
    "# Train Function\n",
    "def train_model(model, train_loader, device, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    # Initiate a loss monitor\n",
    "    train_loss = []\n",
    "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning and not supervised classification)\n",
    "    for i, (images, labels) in enumerate(train_loader): # the variable `labels` will be used for customised training\n",
    "        # reshape input\n",
    "        if (i % (len(train_loader) // 10) == 0):\n",
    "            print(f\"Completed {i} batches\")\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        images = images.float()\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # predict the class\n",
    "        predicted = model(images)\n",
    "        loss = loss_fn(predicted, labels)\n",
    "        # Backward pass (back propagation)\n",
    "        loss.backward()\n",
    "        optimizer.step()       \n",
    "        train_loss.append(loss.detach().cpu().numpy())\n",
    "    return np.mean(train_loss), train_loss\n",
    "\n",
    "# Testing Function\n",
    "def test_model(model, test_loader, device, loss_fn, input_dim=(-1,1,28,28)):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "    model.eval()\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        # Define the lists to store the outputs for each batch\n",
    "        predicted = []\n",
    "        actual = []\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            # reshape input\n",
    "            # images = torch.reshape(images,input_dim)\n",
    "            if (i % (len(val_loader) // 5) == 0):\n",
    "                print(f\"Completed {i} batches\")\n",
    "            images = images.float()\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ## predict the label\n",
    "            pred = model(images)\n",
    "            # Append the network output and the original image to the lists\n",
    "            predicted.append(pred.cpu())\n",
    "            actual.append(labels.cpu())\n",
    "        # Create a single tensor with all the values in the lists\n",
    "        predicted = torch.cat(predicted)\n",
    "        actual = torch.cat(actual) \n",
    "        # Evaluate global loss\n",
    "        val_loss = loss_fn(predicted, actual)\n",
    "    return val_loss.data, (predicted, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88c9bd6d-37fc-4d84-aefd-c47ed135550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMS\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "lr= 0.5\n",
    "num_epochs = 25\n",
    "batch_size = 20\n",
    "optim = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=2*10**-5, momentum=0.9)\n",
    "lr_warmup_decay = 0.01\n",
    "lr_warmup_epochs = 5\n",
    "\n",
    "\n",
    "main_lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=num_epochs - lr_warmup_epochs)\n",
    "warmup_lr_scheduler = torch.optim.lr_scheduler.LinearLR(optim, start_factor=lr_warmup_decay, total_iters=lr_warmup_epochs)\n",
    "lr_scheduler = torch.optim.lr_scheduler.SequentialLR(optim, schedulers=[warmup_lr_scheduler, main_lr_scheduler], milestones=[lr_warmup_epochs])\n",
    "### DATA PREPARATION\n",
    "n_train_samples = len(image_data)\n",
    "val_split = .15\n",
    "\n",
    "\n",
    "train_data, val_data = random_split(image_dataset, [int(n_train_samples*(1-val_split))+1, int(n_train_samples*val_split)])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,shuffle=True, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size,shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33865560-ea04-4cfb-a519-7c60793f05c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet training started\n",
      "Epoch: 0\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 72 batches\n",
      "Completed 144 batches\n",
      "Completed 216 batches\n",
      "Completed 288 batches\n",
      "Completed 360 batches\n",
      "Completed 432 batches\n",
      "Completed 504 batches\n",
      "Completed 576 batches\n",
      "Completed 648 batches\n",
      "Completed 720 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 25 batches\n",
      "Completed 50 batches\n",
      "Completed 75 batches\n",
      "Completed 100 batches\n",
      "Completed 125 batches\n",
      "New Best Model with loss 0.36477890610694885\n",
      "Epoch 1/25 : train loss 0.382 \t val loss 0.365\n",
      "Epoch 0 training done in 1164.667 seconds!\n",
      "Epoch: 1\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 72 batches\n",
      "Completed 144 batches\n",
      "Completed 216 batches\n",
      "Completed 288 batches\n",
      "Completed 360 batches\n",
      "Completed 432 batches\n",
      "Completed 504 batches\n",
      "Completed 576 batches\n",
      "Completed 648 batches\n",
      "Completed 720 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 25 batches\n",
      "Completed 50 batches\n",
      "Completed 75 batches\n",
      "Completed 100 batches\n",
      "Completed 125 batches\n",
      "New Best Model with loss 0.3614961504936218\n",
      "Epoch 2/25 : train loss 0.354 \t val loss 0.361\n",
      "Epoch 1 training done in 1295.241 seconds!\n",
      "Epoch: 2\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 72 batches\n",
      "Completed 144 batches\n",
      "Completed 216 batches\n",
      "Completed 288 batches\n",
      "Completed 360 batches\n",
      "Completed 432 batches\n",
      "Completed 504 batches\n",
      "Completed 576 batches\n",
      "Completed 648 batches\n",
      "Completed 720 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 25 batches\n",
      "Completed 50 batches\n",
      "Completed 75 batches\n",
      "Completed 100 batches\n",
      "Completed 125 batches\n",
      "New Best Model with loss 0.3327290713787079\n",
      "Epoch 3/25 : train loss 0.334 \t val loss 0.333\n",
      "Epoch 2 training done in 1269.646 seconds!\n",
      "Epoch: 3\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 72 batches\n",
      "Completed 144 batches\n",
      "Completed 216 batches\n",
      "Completed 288 batches\n",
      "Completed 360 batches\n",
      "Completed 432 batches\n",
      "Completed 504 batches\n",
      "Completed 576 batches\n",
      "Completed 648 batches\n",
      "Completed 720 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 25 batches\n",
      "Completed 50 batches\n",
      "Completed 75 batches\n",
      "Completed 100 batches\n",
      "Completed 125 batches\n",
      "New Best Model with loss 0.28864264488220215\n",
      "Epoch 4/25 : train loss 0.302 \t val loss 0.289\n",
      "Epoch 3 training done in 1237.164 seconds!\n",
      "Epoch: 4\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 72 batches\n",
      "Completed 144 batches\n",
      "Completed 216 batches\n",
      "Completed 288 batches\n",
      "Completed 360 batches\n",
      "Completed 432 batches\n",
      "Completed 504 batches\n",
      "Completed 576 batches\n",
      "Completed 648 batches\n",
      "Completed 720 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 25 batches\n",
      "Completed 50 batches\n",
      "Completed 75 batches\n",
      "Completed 100 batches\n",
      "Completed 125 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lachyankar.a/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Best Model with loss 0.2598411440849304\n",
      "Epoch 5/25 : train loss 0.245 \t val loss 0.260\n",
      "Epoch 4 training done in 1129.332 seconds!\n",
      "Epoch: 5\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 72 batches\n",
      "Completed 144 batches\n",
      "Completed 216 batches\n",
      "Completed 288 batches\n",
      "Completed 360 batches\n",
      "Completed 432 batches\n",
      "Completed 504 batches\n",
      "Completed 576 batches\n",
      "Completed 648 batches\n",
      "Completed 720 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 25 batches\n",
      "Completed 50 batches\n",
      "Completed 75 batches\n",
      "Completed 100 batches\n",
      "Completed 125 batches\n",
      "New Best Model with loss 0.2135738730430603\n",
      "Epoch 6/25 : train loss 0.182 \t val loss 0.214\n",
      "Epoch 5 training done in 1189.023 seconds!\n",
      "Epoch: 6\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 72 batches\n",
      "Completed 144 batches\n",
      "Completed 216 batches\n",
      "Completed 288 batches\n",
      "Completed 360 batches\n",
      "Completed 432 batches\n",
      "Completed 504 batches\n",
      "Completed 576 batches\n",
      "Completed 648 batches\n",
      "Completed 720 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 25 batches\n",
      "Completed 50 batches\n",
      "Completed 75 batches\n",
      "Completed 100 batches\n",
      "Completed 125 batches\n",
      "New Best Model with loss 0.16257700324058533\n",
      "Epoch 7/25 : train loss 0.109 \t val loss 0.163\n",
      "Epoch 6 training done in 1245.145 seconds!\n",
      "Epoch: 7\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 72 batches\n",
      "Completed 144 batches\n",
      "Completed 216 batches\n",
      "Completed 288 batches\n",
      "Completed 360 batches\n",
      "Completed 432 batches\n",
      "Completed 504 batches\n",
      "Completed 576 batches\n",
      "Completed 648 batches\n",
      "Completed 720 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 25 batches\n",
      "Completed 50 batches\n",
      "Completed 75 batches\n",
      "Completed 100 batches\n",
      "Completed 125 batches\n",
      "New Best Model with loss 0.1580505669116974\n",
      "Epoch 8/25 : train loss 0.070 \t val loss 0.158\n",
      "Epoch 7 training done in 1116.843 seconds!\n",
      "Epoch: 8\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 72 batches\n",
      "Completed 144 batches\n",
      "Completed 216 batches\n",
      "Completed 288 batches\n",
      "Completed 360 batches\n",
      "Completed 432 batches\n",
      "Completed 504 batches\n",
      "Completed 576 batches\n",
      "Completed 648 batches\n",
      "Completed 720 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 25 batches\n",
      "Completed 50 batches\n",
      "Completed 75 batches\n",
      "Completed 100 batches\n",
      "Completed 125 batches\n",
      "New Best Model with loss 0.0633934736251831\n",
      "Epoch 9/25 : train loss 0.044 \t val loss 0.063\n",
      "Epoch 8 training done in 1244.302 seconds!\n",
      "Epoch: 9\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 72 batches\n",
      "Completed 144 batches\n",
      "Completed 216 batches\n",
      "Completed 288 batches\n",
      "Completed 360 batches\n",
      "Completed 432 batches\n",
      "Completed 504 batches\n",
      "Completed 576 batches\n",
      "Completed 648 batches\n",
      "Completed 720 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 25 batches\n",
      "Completed 50 batches\n",
      "Completed 75 batches\n",
      "Completed 100 batches\n",
      "Completed 125 batches\n",
      "Epoch 10/25 : train loss 0.029 \t val loss 0.175\n",
      "Epoch 9 training done in 1199.428 seconds!\n",
      "Epoch: 10\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 72 batches\n",
      "Completed 144 batches\n",
      "Completed 216 batches\n",
      "Completed 288 batches\n",
      "Completed 360 batches\n",
      "Completed 432 batches\n",
      "Completed 504 batches\n",
      "Completed 576 batches\n",
      "Completed 648 batches\n",
      "Completed 720 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 25 batches\n",
      "Completed 50 batches\n",
      "Completed 75 batches\n",
      "Completed 100 batches\n",
      "Completed 125 batches\n",
      "New Best Model with loss 0.04186283424496651\n",
      "Epoch 11/25 : train loss 0.022 \t val loss 0.042\n",
      "Epoch 10 training done in 1150.471 seconds!\n",
      "Epoch: 11\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 72 batches\n",
      "Completed 144 batches\n",
      "Completed 216 batches\n",
      "Completed 288 batches\n",
      "Completed 360 batches\n",
      "Completed 432 batches\n",
      "Completed 504 batches\n",
      "Completed 576 batches\n",
      "Completed 648 batches\n",
      "Completed 720 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 25 batches\n",
      "Completed 50 batches\n",
      "Completed 75 batches\n",
      "Completed 100 batches\n",
      "Completed 125 batches\n",
      "New Best Model with loss 0.014425747096538544\n",
      "Epoch 12/25 : train loss 0.013 \t val loss 0.014\n",
      "Epoch 11 training done in 1147.646 seconds!\n",
      "Epoch: 12\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 72 batches\n",
      "Completed 144 batches\n",
      "Completed 216 batches\n",
      "Completed 288 batches\n",
      "Completed 360 batches\n",
      "Completed 432 batches\n",
      "Completed 504 batches\n",
      "Completed 576 batches\n",
      "Completed 648 batches\n",
      "Completed 720 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 25 batches\n",
      "Completed 50 batches\n",
      "Completed 75 batches\n",
      "Completed 100 batches\n",
      "Completed 125 batches\n",
      "New Best Model with loss 0.01070729736238718\n",
      "Epoch 13/25 : train loss 0.008 \t val loss 0.011\n",
      "Epoch 12 training done in 1418.429 seconds!\n",
      "Epoch: 13\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 72 batches\n",
      "Completed 144 batches\n",
      "Completed 216 batches\n",
      "Completed 288 batches\n",
      "Completed 360 batches\n",
      "Completed 432 batches\n",
      "Completed 504 batches\n",
      "Completed 576 batches\n",
      "Completed 648 batches\n",
      "Completed 720 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 25 batches\n",
      "Completed 50 batches\n",
      "Completed 75 batches\n",
      "Completed 100 batches\n",
      "Completed 125 batches\n",
      "New Best Model with loss 0.0017917545046657324\n",
      "Epoch 14/25 : train loss 0.004 \t val loss 0.002\n",
      "Epoch 13 training done in 1416.217 seconds!\n",
      "Epoch: 14\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 72 batches\n",
      "Completed 144 batches\n",
      "Completed 216 batches\n",
      "Completed 288 batches\n",
      "Completed 360 batches\n",
      "Completed 432 batches\n",
      "Completed 504 batches\n",
      "Completed 576 batches\n",
      "Completed 648 batches\n",
      "Completed 720 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 25 batches\n",
      "Completed 50 batches\n",
      "Completed 75 batches\n",
      "Completed 100 batches\n",
      "Completed 125 batches\n",
      "New Best Model with loss 0.0009713223553262651\n",
      "Epoch 15/25 : train loss 0.001 \t val loss 0.001\n",
      "Epoch 14 training done in 1439.873 seconds!\n",
      "Epoch: 15\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 72 batches\n",
      "Completed 144 batches\n",
      "Completed 216 batches\n",
      "Completed 288 batches\n",
      "Completed 360 batches\n",
      "Completed 432 batches\n",
      "Completed 504 batches\n",
      "Completed 576 batches\n",
      "Completed 648 batches\n",
      "Completed 720 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 25 batches\n",
      "Completed 50 batches\n",
      "Completed 75 batches\n",
      "Completed 100 batches\n",
      "Completed 125 batches\n",
      "New Best Model with loss 0.0009443191811442375\n",
      "Epoch 16/25 : train loss 0.001 \t val loss 0.001\n",
      "Epoch 15 training done in 1446.444 seconds!\n",
      "Epoch: 16\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 72 batches\n",
      "Completed 144 batches\n",
      "Completed 216 batches\n",
      "Completed 288 batches\n",
      "Completed 360 batches\n",
      "Completed 432 batches\n",
      "Completed 504 batches\n",
      "Completed 576 batches\n",
      "Completed 648 batches\n",
      "Completed 720 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 25 batches\n",
      "Completed 50 batches\n",
      "Completed 75 batches\n",
      "Completed 100 batches\n",
      "Completed 125 batches\n",
      "Epoch 17/25 : train loss 0.001 \t val loss 0.001\n",
      "Epoch 16 training done in 1450.360 seconds!\n",
      "Epoch: 17\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 72 batches\n",
      "Completed 144 batches\n",
      "Completed 216 batches\n",
      "Completed 288 batches\n",
      "Completed 360 batches\n",
      "Completed 432 batches\n",
      "Completed 504 batches\n",
      "Completed 576 batches\n",
      "Completed 648 batches\n",
      "Completed 720 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 25 batches\n",
      "Completed 50 batches\n",
      "Completed 75 batches\n",
      "Completed 100 batches\n",
      "Completed 125 batches\n",
      "Epoch 18/25 : train loss 0.001 \t val loss 0.001\n",
      "Epoch 17 training done in 1413.578 seconds!\n",
      "Epoch: 18\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 72 batches\n",
      "Completed 144 batches\n",
      "Completed 216 batches\n",
      "Completed 288 batches\n",
      "Completed 360 batches\n",
      "Completed 432 batches\n",
      "Completed 504 batches\n",
      "Completed 576 batches\n",
      "Completed 648 batches\n",
      "Completed 720 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 25 batches\n",
      "Completed 50 batches\n",
      "Completed 75 batches\n",
      "Completed 100 batches\n",
      "Completed 125 batches\n",
      "New Best Model with loss 0.0009133503190241754\n",
      "Epoch 19/25 : train loss 0.001 \t val loss 0.001\n",
      "Epoch 18 training done in 1438.056 seconds!\n",
      "Epoch: 19\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 72 batches\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m### Training \u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Started\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m epoch_train_loss, batch_train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Started\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m### Validation  (use the testing function)\u001b[39;00m\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, device, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      7\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Iterate the dataloader (we do not need the label values, this is unsupervised learning and not supervised classification)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader): \u001b[38;5;66;03m# the variable `labels` will be used for customised training\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# reshape input\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(train_loader) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m batches\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:61\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:143\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    140\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:143\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    140\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:120\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:163\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    161\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    162\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "### Pytorch Native TRAIN LOOP\n",
    "\n",
    "print('ResNet training started')\n",
    "history = {'train_loss':[],'val_loss':[], \"batch_train_loss\": []}\n",
    "start_time = datetime.datetime.now()\n",
    "folder_date = start_time.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "model_ckpt_path = f\"ResNet_model_train_{folder_date}\"\n",
    "\n",
    "if not os.path.exists(f\"model_ckpts/{model_ckpt_path}\"):\n",
    "    os.mkdir(f\"model_ckpts/{model_ckpt_path}\")\n",
    "    \n",
    "best_val_loss = 10**10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    epoch_start_time = datetime.datetime.now()\n",
    "    ### Training \n",
    "    print(\"Training Started\")\n",
    "    epoch_train_loss, batch_train_loss = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        device=device,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optim)\n",
    "    print(\"Validation Started\")\n",
    "    ### Validation  (use the testing function)\n",
    "    val_loss, predictions = test_model(\n",
    "        model=model,\n",
    "        test_loader=val_loader,\n",
    "        device=device,\n",
    "        loss_fn=loss_fn)\n",
    "    lr_scheduler.step()\n",
    "    # Print Losses \n",
    "    if (val_loss < best_val_loss):\n",
    "        print(f\"New Best Model with loss {val_loss}\")\n",
    "        torch.save(model.state_dict(), f\"model_ckpts/{model_ckpt_path}/{epoch}_model_loss_{val_loss}.ckpt\")\n",
    "        best_val_loss = val_loss\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} : train loss {epoch_train_loss:.3f} \\t val loss {val_loss:.3f}')\n",
    "    history['train_loss'].append(epoch_train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['batch_train_loss'].extend(batch_train_loss)\n",
    "    with open(f\"model_ckpts/{model_ckpt_path}/epoch_{epoch}_model_results.pickle\", 'wb') as f:\n",
    "        pickle.dump(history, f)\n",
    "    print(f'Epoch {epoch} training done in {(datetime.datetime.now()-epoch_start_time).total_seconds():.3f} seconds!')\n",
    "    \n",
    "with open(f\"model_ckpts/{model_ckpt_path}/model_results.pickle\", 'wb') as f:\n",
    "    pickle.dump(history, f)\n",
    "    \n",
    "print(f'ResNet training done in {(datetime.datetime.now()-start_time).total_seconds():.3f} seconds!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa42a713-d059-4082-8182-30fed9e3b908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(16,4))\n",
    "\n",
    "ax1.plot(history['train_loss'])\n",
    "ax1.title.set_text('Epoch Train Loss')\n",
    "\n",
    "ax2.plot(history['batch_train_loss'])\n",
    "ax2.title.set_text('Batch Train Loss')\n",
    "\n",
    "ax3.plot(history['val_loss'])\n",
    "ax3.title.set_text('Epoch Validation Loss')\n",
    "\n",
    "plt.savefig('resnet_loss_graphs.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a0dabb0-dbef-4d2a-acbf-633aaf8648c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303 Samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-50 and are newly initialized because the shapes did not match:\n",
      "- classifier.1.weight: found shape torch.Size([1000, 2048]) in the checkpoint and torch.Size([8, 2048]) in the model instantiated\n",
      "- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 0 batches\n"
     ]
    }
   ],
   "source": [
    "path = \"project_data/project_analysis/sag_test\"\n",
    "image_data_test = ImageFolder(root=path, target_transform=lambda x: np.array([1,1,0,0,0,0,0,0]))\n",
    "print(f\"{len(image_data_test)} Samples\")\n",
    "\n",
    "image_dataset_test = ImageDataset(image_data_test, base_augmentations)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(image_dataset_test, batch_size=batch_size,shuffle=False, num_workers=0)\n",
    "\n",
    "model = ResNetForMultiClassPrediction()\n",
    "model.load_state_dict(torch.load('model_ckpts/ResNet_model_train_2022_12_04_16_39_28/18_model_loss_0.0009133503190241754.ckpt'))\n",
    "model.to(device)\n",
    "\n",
    "test_loss, predictions = test_model(\n",
    "    model=model,\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    loss_fn=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b52a928d-d552-43d9-ae59-336a8858e68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_arr = predictions[0].numpy()\n",
    "actual_arr = predictions[1].numpy()\n",
    "thresholded_pred = np.where(pred_arr > 0.5, 1, 0)\n",
    "correct = (thresholded_pred == actual_arr).all(axis=1).nonzero()[0]\n",
    "\n",
    "for i,index in enumerate(correct):\n",
    "    im = image_data_test[index][0]\n",
    "    im.save(f'correct_resnet_{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "752333c1-39a8-4803-8ffe-25dd7add66da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(thresholded_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be39f6c2-e8f3-49ad-a647-f8340a7799da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 0 batches\n",
      "Completed 25 batches\n",
      "Completed 50 batches\n",
      "Completed 75 batches\n"
     ]
    }
   ],
   "source": [
    "val_loss, predictions = test_model(\n",
    "    model=model,\n",
    "    test_loader=val_loader,\n",
    "    device=device,\n",
    "    loss_fn=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8780f2-3daf-4bed-aa8c-ac0875c3388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_arr = predictions[0].numpy()\n",
    "actual_arr = predictions[1].numpy()\n",
    "thresholded_pred = np.where(pred_arr > 0.5, 1, 0)\n",
    "correct = (thresholded_pred == actual_arr).all(axis=1).nonzero()[0]\n",
    "\n",
    "for i,index in enumerate(correct):\n",
    "    im = image_data_test[index][0]\n",
    "    im.save(f'correct_resnet_val_{i}.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
