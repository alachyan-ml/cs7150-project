{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38d59266-91c5-480e-b709-9d80cc8dfc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q transformers\n",
    "#!pip install -U albumentations\n",
    "# !pip install torchsummary\n",
    "# !pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a6fa2d8b-5bdb-42e0-9bee-20f3035e4c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor, ToPILImage\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader,random_split,Subset\n",
    "from transformers import ViTForImageClassification\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import albumentations as A\n",
    "import albumentations.pytorch as ATorch\n",
    "from albumentations.augmentations.geometric.transforms import PadIfNeeded\n",
    "from albumentations.augmentations.transforms import Normalize\n",
    "from albumentations.augmentations.geometric.resize import Resize\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import gc\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "545aba18-9e29-48a8-87e3-3b4e02240363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(f'Selected device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a73afdc-30e7-406f-b127-7ab250323aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16947 Samples\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"project_data/train.csv\")\n",
    "\n",
    "def get_frac_data_from_patient(target):\n",
    "    patient_index = list(image_data.class_to_idx.values()).index(target)\n",
    "    patient_name = list(image_data.class_to_idx.keys())[patient_index]\n",
    "    row_for_pt = train_df[train_df[\"StudyInstanceUID\"] == patient_name]\n",
    "    return row_for_pt.iloc[:,1:].values\n",
    "\n",
    "\n",
    "path = \"project_data/project_analysis/sag\"\n",
    "image_data = ImageFolder(root=path, target_transform= get_frac_data_from_patient)\n",
    "print(f\"{len(image_data)} Samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bb97451-2ceb-4af6-8a7a-de5101d41c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_shapes(dataset):\n",
    "    tensor_t = ToTensor()\n",
    "    shapes = []\n",
    "    for sample in dataset:\n",
    "        shapes.append(sample[0].size)\n",
    "    return torch.Tensor(shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ad483d6-6009-4012-aa6b-12ac5b993f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_sequence(dataset):\n",
    "    i = 0\n",
    "    img_data_list = []\n",
    "    for sample in random.sample(range(len(dataset)), len(dataset) // 4):\n",
    "        # print(sample[0])\n",
    "        img = dataset[sample]\n",
    "        img_data = img[0].getdata()\n",
    "        img_data_list.append(img_data)\n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "    return np.concatenate(img_data_list, axis=0) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f41e506-5fc6-4256-a707-88f295702048",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_file = 'image_stats.pickle'\n",
    "\n",
    "if not os.path.exists(stat_file):\n",
    "    print(\"Getting Shapes\")\n",
    "    shapes = get_image_shapes(image_data)\n",
    "    print(f\"Max Height: {torch.max(shapes[:,1])}, Max Width: {torch.max(shapes[:,0])}\")\n",
    "    print(\"Getting mean and std\")\n",
    "    data = get_image_sequence(image_data)\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std = np.std(data, axis=0)\n",
    "    print(f\"mean: {mean}, std:{std}\")\n",
    "    dataset_stats = {\"mean\": mean, \"std\": std, \"shapes\": shapes}\n",
    "    with open(stat_file, 'wb') as f:\n",
    "        pickle.dump(dataset_stats, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open(stat_file, 'rb') as handle:\n",
    "        dataset_stats = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4437033-fb31-43b6-8eb7-0dfecc10ff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dataset, transform=None):\n",
    "        self.image_data = image_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.image_data[idx]\n",
    "        image = np.array(image)\n",
    "        label = label.astype(np.float32)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "        return image, np.squeeze(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf14f2f4-1d30-485c-9a84-d96a5d26ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_augmentations = A.Compose([\n",
    "    PadIfNeeded(int(torch.max(dataset_stats[\"shapes\"][:,1]).item()), int(torch.max(dataset_stats[\"shapes\"][:,0]).item())),\n",
    "    A.Normalize(mean=dataset_stats[\"mean\"].tolist(), std=dataset_stats[\"std\"].tolist(), max_pixel_value=1),\n",
    "    Resize(384,384),\n",
    "    ATorch.transforms.ToTensorV2(),\n",
    "])\n",
    "    \n",
    "image_dataset = ImageDataset(image_data, base_augmentations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dca3a7c7-f2e6-4490-8cac-5ed1171b03fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-384 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([8, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForMultiClassPrediction(\n",
       "  (ViT): ViTForImageClassification(\n",
       "    (vit): ViTModel(\n",
       "      (embeddings): ViTEmbeddings(\n",
       "        (patch_embeddings): ViTPatchEmbeddings(\n",
       "          (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): ViTEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): ViTLayer(\n",
       "            (attention): ViTAttention(\n",
       "              (attention): ViTSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): ViTSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ViTIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): ViTOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (1): ViTLayer(\n",
       "            (attention): ViTAttention(\n",
       "              (attention): ViTSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): ViTSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ViTIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): ViTOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (2): ViTLayer(\n",
       "            (attention): ViTAttention(\n",
       "              (attention): ViTSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): ViTSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ViTIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): ViTOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (3): ViTLayer(\n",
       "            (attention): ViTAttention(\n",
       "              (attention): ViTSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): ViTSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ViTIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): ViTOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (4): ViTLayer(\n",
       "            (attention): ViTAttention(\n",
       "              (attention): ViTSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): ViTSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ViTIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): ViTOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (5): ViTLayer(\n",
       "            (attention): ViTAttention(\n",
       "              (attention): ViTSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): ViTSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ViTIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): ViTOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (6): ViTLayer(\n",
       "            (attention): ViTAttention(\n",
       "              (attention): ViTSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): ViTSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ViTIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): ViTOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (7): ViTLayer(\n",
       "            (attention): ViTAttention(\n",
       "              (attention): ViTSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): ViTSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ViTIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): ViTOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (8): ViTLayer(\n",
       "            (attention): ViTAttention(\n",
       "              (attention): ViTSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): ViTSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ViTIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): ViTOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (9): ViTLayer(\n",
       "            (attention): ViTAttention(\n",
       "              (attention): ViTSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): ViTSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ViTIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): ViTOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (10): ViTLayer(\n",
       "            (attention): ViTAttention(\n",
       "              (attention): ViTSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): ViTSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ViTIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): ViTOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (11): ViTLayer(\n",
       "            (attention): ViTAttention(\n",
       "              (attention): ViTSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): ViTSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ViTIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): ViTOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (classifier): Linear(in_features=768, out_features=8, bias=True)\n",
       "  )\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Model Declaration\n",
    "\n",
    "class ViTForMultiClassPrediction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ViTForMultiClassPrediction, self).__init__()\n",
    "        self.ViT = ViTForImageClassification.from_pretrained('google/vit-base-patch16-384', num_labels=8, ignore_mismatched_sizes=True)\n",
    "        # Add Sigmoid for output between 0 and 1\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ViT(x).logits\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = ViTForMultiClassPrediction()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b4e11a6-da0e-4989-9c1b-b6b50cf34678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Test functions (Taken from HW3 of CS7150 HWs)\n",
    "\n",
    "# Train Function\n",
    "def train_model(model, train_loader, device, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    # Initiate a loss monitor\n",
    "    train_loss = []\n",
    "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning and not supervised classification)\n",
    "    for i, (images, labels) in enumerate(train_loader): # the variable `labels` will be used for customised training\n",
    "        # reshape input\n",
    "        if (i % (len(train_loader) // 10) == 0):\n",
    "            print(f\"Completed {i} batches\")\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        images = images.float()\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # predict the class\n",
    "        predicted = model(images)\n",
    "        loss = loss_fn(predicted, labels)\n",
    "        # Backward pass (back propagation)\n",
    "        loss.backward()\n",
    "        optimizer.step()       \n",
    "        train_loss.append(loss.detach().cpu().numpy())\n",
    "    return np.mean(train_loss), train_loss\n",
    "\n",
    "# Testing Function\n",
    "def test_model(model, test_loader, device, loss_fn, input_dim=(-1,1,28,28)):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "    model.eval()\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        # Define the lists to store the outputs for each batch\n",
    "        predicted = []\n",
    "        actual = []\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            # reshape input\n",
    "            # images = torch.reshape(images,input_dim)\n",
    "            if (i % (len(val_loader) // 5) == 0):\n",
    "                print(f\"Completed {i} batches\")\n",
    "            images = images.float()\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ## predict the label\n",
    "            pred = model(images)\n",
    "            # Append the network output and the original image to the lists\n",
    "            predicted.append(pred.cpu())\n",
    "            actual.append(labels.cpu())\n",
    "        # Create a single tensor with all the values in the lists\n",
    "        predicted = torch.cat(predicted)\n",
    "        actual = torch.cat(actual) \n",
    "        # Evaluate global loss\n",
    "        val_loss = loss_fn(predicted, actual)\n",
    "    return val_loss.data, (predicted, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88c9bd6d-37fc-4d84-aefd-c47ed135550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMS\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "lr=0.00003\n",
    "num_epochs = 50\n",
    "batch_size = 12\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "lr_scheduler = torch.optim.lr_scheduler.LinearLR(optim)\n",
    "\n",
    "### DATA PREPARATION\n",
    "n_train_samples = len(image_data)\n",
    "val_split = .15\n",
    "\n",
    "\n",
    "train_data, val_data = random_split(image_dataset, [int(n_train_samples*(1-val_split))+1, int(n_train_samples*val_split)])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,shuffle=True, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size,shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33865560-ea04-4cfb-a519-7c60793f05c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT training started\n",
      "Epoch: 0\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 120 batches\n",
      "Completed 240 batches\n",
      "Completed 360 batches\n",
      "Completed 480 batches\n",
      "Completed 600 batches\n",
      "Completed 720 batches\n",
      "Completed 840 batches\n",
      "Completed 960 batches\n",
      "Completed 1080 batches\n",
      "Completed 1200 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 42 batches\n",
      "Completed 84 batches\n",
      "Completed 126 batches\n",
      "Completed 168 batches\n",
      "Completed 210 batches\n",
      "New Best Model with loss 0.3524111211299896\n",
      "Epoch 1/50 : train loss 0.366 \t val loss 0.352\n",
      "Epoch 0 training done in 984.439 seconds!\n",
      "Epoch: 1\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 120 batches\n",
      "Completed 240 batches\n",
      "Completed 360 batches\n",
      "Completed 480 batches\n",
      "Completed 600 batches\n",
      "Completed 720 batches\n",
      "Completed 840 batches\n",
      "Completed 960 batches\n",
      "Completed 1080 batches\n",
      "Completed 1200 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 42 batches\n",
      "Completed 84 batches\n",
      "Completed 126 batches\n",
      "Completed 168 batches\n",
      "Completed 210 batches\n",
      "New Best Model with loss 0.3458956778049469\n",
      "Epoch 2/50 : train loss 0.353 \t val loss 0.346\n",
      "Epoch 1 training done in 1022.595 seconds!\n",
      "Epoch: 2\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 120 batches\n",
      "Completed 240 batches\n",
      "Completed 360 batches\n",
      "Completed 480 batches\n",
      "Completed 600 batches\n",
      "Completed 720 batches\n",
      "Completed 840 batches\n",
      "Completed 960 batches\n",
      "Completed 1080 batches\n",
      "Completed 1200 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 42 batches\n",
      "Completed 84 batches\n",
      "Completed 126 batches\n",
      "Completed 168 batches\n",
      "Completed 210 batches\n",
      "New Best Model with loss 0.3209605813026428\n",
      "Epoch 3/50 : train loss 0.329 \t val loss 0.321\n",
      "Epoch 2 training done in 1051.173 seconds!\n",
      "Epoch: 3\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 120 batches\n",
      "Completed 240 batches\n",
      "Completed 360 batches\n",
      "Completed 480 batches\n",
      "Completed 600 batches\n",
      "Completed 720 batches\n",
      "Completed 840 batches\n",
      "Completed 960 batches\n",
      "Completed 1080 batches\n",
      "Completed 1200 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 42 batches\n",
      "Completed 84 batches\n",
      "Completed 126 batches\n",
      "Completed 168 batches\n",
      "Completed 210 batches\n",
      "New Best Model with loss 0.2862994074821472\n",
      "Epoch 4/50 : train loss 0.278 \t val loss 0.286\n",
      "Epoch 3 training done in 1054.205 seconds!\n",
      "Epoch: 4\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 120 batches\n",
      "Completed 240 batches\n",
      "Completed 360 batches\n",
      "Completed 480 batches\n",
      "Completed 600 batches\n",
      "Completed 720 batches\n",
      "Completed 840 batches\n",
      "Completed 960 batches\n",
      "Completed 1080 batches\n",
      "Completed 1200 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 42 batches\n",
      "Completed 84 batches\n",
      "Completed 126 batches\n",
      "Completed 168 batches\n",
      "Completed 210 batches\n",
      "New Best Model with loss 0.2539655864238739\n",
      "Epoch 5/50 : train loss 0.208 \t val loss 0.254\n",
      "Epoch 4 training done in 1056.832 seconds!\n",
      "Epoch: 5\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 120 batches\n",
      "Completed 240 batches\n",
      "Completed 360 batches\n",
      "Completed 480 batches\n",
      "Completed 600 batches\n",
      "Completed 720 batches\n",
      "Completed 840 batches\n",
      "Completed 960 batches\n",
      "Completed 1080 batches\n",
      "Completed 1200 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 42 batches\n",
      "Completed 84 batches\n",
      "Completed 126 batches\n",
      "Completed 168 batches\n",
      "Completed 210 batches\n",
      "New Best Model with loss 0.22142387926578522\n",
      "Epoch 6/50 : train loss 0.142 \t val loss 0.221\n",
      "Epoch 5 training done in 1094.991 seconds!\n",
      "Epoch: 6\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 120 batches\n",
      "Completed 240 batches\n",
      "Completed 360 batches\n",
      "Completed 480 batches\n",
      "Completed 600 batches\n",
      "Completed 720 batches\n",
      "Completed 840 batches\n",
      "Completed 960 batches\n",
      "Completed 1080 batches\n",
      "Completed 1200 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 42 batches\n",
      "Completed 84 batches\n",
      "Completed 126 batches\n",
      "Completed 168 batches\n",
      "Completed 210 batches\n",
      "New Best Model with loss 0.1959216594696045\n",
      "Epoch 7/50 : train loss 0.086 \t val loss 0.196\n",
      "Epoch 6 training done in 1092.432 seconds!\n",
      "Epoch: 7\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 120 batches\n",
      "Completed 240 batches\n",
      "Completed 360 batches\n",
      "Completed 480 batches\n",
      "Completed 600 batches\n",
      "Completed 720 batches\n",
      "Completed 840 batches\n",
      "Completed 960 batches\n",
      "Completed 1080 batches\n",
      "Completed 1200 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 42 batches\n",
      "Completed 84 batches\n",
      "Completed 126 batches\n",
      "Completed 168 batches\n",
      "Completed 210 batches\n",
      "New Best Model with loss 0.19459381699562073\n",
      "Epoch 8/50 : train loss 0.053 \t val loss 0.195\n",
      "Epoch 7 training done in 834.455 seconds!\n",
      "Epoch: 8\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 120 batches\n",
      "Completed 240 batches\n",
      "Completed 360 batches\n",
      "Completed 480 batches\n",
      "Completed 600 batches\n",
      "Completed 720 batches\n",
      "Completed 840 batches\n",
      "Completed 960 batches\n",
      "Completed 1080 batches\n",
      "Completed 1200 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 42 batches\n",
      "Completed 84 batches\n",
      "Completed 126 batches\n",
      "Completed 168 batches\n",
      "Completed 210 batches\n",
      "New Best Model with loss 0.19310231506824493\n",
      "Epoch 9/50 : train loss 0.036 \t val loss 0.193\n",
      "Epoch 8 training done in 1020.893 seconds!\n",
      "Epoch: 9\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 120 batches\n",
      "Completed 240 batches\n",
      "Completed 360 batches\n",
      "Completed 480 batches\n",
      "Completed 600 batches\n",
      "Completed 720 batches\n",
      "Completed 840 batches\n",
      "Completed 960 batches\n",
      "Completed 1080 batches\n",
      "Completed 1200 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 42 batches\n",
      "Completed 84 batches\n",
      "Completed 126 batches\n",
      "Completed 168 batches\n",
      "Completed 210 batches\n",
      "Epoch 10/50 : train loss 0.029 \t val loss 0.228\n",
      "Epoch 9 training done in 1121.656 seconds!\n",
      "Epoch: 10\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 120 batches\n",
      "Completed 240 batches\n",
      "Completed 360 batches\n",
      "Completed 480 batches\n",
      "Completed 600 batches\n",
      "Completed 720 batches\n",
      "Completed 840 batches\n",
      "Completed 960 batches\n",
      "Completed 1080 batches\n",
      "Completed 1200 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 42 batches\n",
      "Completed 84 batches\n",
      "Completed 126 batches\n",
      "Completed 168 batches\n",
      "Completed 210 batches\n",
      "Epoch 11/50 : train loss 0.019 \t val loss 0.238\n",
      "Epoch 10 training done in 1119.508 seconds!\n",
      "Epoch: 11\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 120 batches\n",
      "Completed 240 batches\n",
      "Completed 360 batches\n",
      "Completed 480 batches\n",
      "Completed 600 batches\n",
      "Completed 720 batches\n",
      "Completed 840 batches\n",
      "Completed 960 batches\n",
      "Completed 1080 batches\n",
      "Completed 1200 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 42 batches\n",
      "Completed 84 batches\n",
      "Completed 126 batches\n",
      "Completed 168 batches\n",
      "Completed 210 batches\n",
      "Epoch 12/50 : train loss 0.019 \t val loss 0.248\n",
      "Epoch 11 training done in 1129.495 seconds!\n",
      "Epoch: 12\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 120 batches\n",
      "Completed 240 batches\n",
      "Completed 360 batches\n",
      "Completed 480 batches\n",
      "Completed 600 batches\n",
      "Completed 720 batches\n",
      "Completed 840 batches\n",
      "Completed 960 batches\n",
      "Completed 1080 batches\n",
      "Completed 1200 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 42 batches\n",
      "Completed 84 batches\n",
      "Completed 126 batches\n",
      "Completed 168 batches\n",
      "Completed 210 batches\n",
      "Epoch 13/50 : train loss 0.017 \t val loss 0.196\n",
      "Epoch 12 training done in 1144.400 seconds!\n",
      "Epoch: 13\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 120 batches\n",
      "Completed 240 batches\n",
      "Completed 360 batches\n",
      "Completed 480 batches\n",
      "Completed 600 batches\n",
      "Completed 720 batches\n",
      "Completed 840 batches\n",
      "Completed 960 batches\n",
      "Completed 1080 batches\n",
      "Completed 1200 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 42 batches\n",
      "Completed 84 batches\n",
      "Completed 126 batches\n",
      "Completed 168 batches\n",
      "Completed 210 batches\n",
      "Epoch 14/50 : train loss 0.012 \t val loss 0.249\n",
      "Epoch 13 training done in 1142.008 seconds!\n",
      "Epoch: 14\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 120 batches\n",
      "Completed 240 batches\n",
      "Completed 360 batches\n",
      "Completed 480 batches\n",
      "Completed 600 batches\n",
      "Completed 720 batches\n",
      "Completed 840 batches\n",
      "Completed 960 batches\n",
      "Completed 1080 batches\n",
      "Completed 1200 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 42 batches\n",
      "Completed 84 batches\n",
      "Completed 126 batches\n",
      "Completed 168 batches\n",
      "Completed 210 batches\n",
      "Epoch 15/50 : train loss 0.012 \t val loss 0.282\n",
      "Epoch 14 training done in 1133.000 seconds!\n",
      "Epoch: 15\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 120 batches\n",
      "Completed 240 batches\n",
      "Completed 360 batches\n",
      "Completed 480 batches\n",
      "Completed 600 batches\n",
      "Completed 720 batches\n",
      "Completed 840 batches\n",
      "Completed 960 batches\n",
      "Completed 1080 batches\n",
      "Completed 1200 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 42 batches\n",
      "Completed 84 batches\n",
      "Completed 126 batches\n",
      "Completed 168 batches\n",
      "Completed 210 batches\n",
      "Epoch 16/50 : train loss 0.011 \t val loss 0.213\n",
      "Epoch 15 training done in 1105.415 seconds!\n",
      "Epoch: 16\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 120 batches\n",
      "Completed 240 batches\n",
      "Completed 360 batches\n",
      "Completed 480 batches\n",
      "Completed 600 batches\n",
      "Completed 720 batches\n",
      "Completed 840 batches\n",
      "Completed 960 batches\n",
      "Completed 1080 batches\n",
      "Completed 1200 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 42 batches\n",
      "Completed 84 batches\n",
      "Completed 126 batches\n",
      "Completed 168 batches\n",
      "Completed 210 batches\n",
      "Epoch 17/50 : train loss 0.013 \t val loss 0.205\n",
      "Epoch 16 training done in 1118.851 seconds!\n",
      "Epoch: 17\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 120 batches\n",
      "Completed 240 batches\n",
      "Completed 360 batches\n",
      "Completed 480 batches\n",
      "Completed 600 batches\n",
      "Completed 720 batches\n",
      "Completed 840 batches\n",
      "Completed 960 batches\n",
      "Completed 1080 batches\n",
      "Completed 1200 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 42 batches\n",
      "Completed 84 batches\n",
      "Completed 126 batches\n",
      "Completed 168 batches\n",
      "Completed 210 batches\n",
      "Epoch 18/50 : train loss 0.008 \t val loss 0.223\n",
      "Epoch 17 training done in 1144.611 seconds!\n",
      "Epoch: 18\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 120 batches\n",
      "Completed 240 batches\n",
      "Completed 360 batches\n",
      "Completed 480 batches\n",
      "Completed 600 batches\n",
      "Completed 720 batches\n",
      "Completed 840 batches\n",
      "Completed 960 batches\n",
      "Completed 1080 batches\n",
      "Completed 1200 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 42 batches\n",
      "Completed 84 batches\n",
      "Completed 126 batches\n",
      "Completed 168 batches\n",
      "Completed 210 batches\n",
      "Epoch 19/50 : train loss 0.010 \t val loss 0.226\n",
      "Epoch 18 training done in 1133.298 seconds!\n",
      "Epoch: 19\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 120 batches\n",
      "Completed 240 batches\n",
      "Completed 360 batches\n",
      "Completed 480 batches\n",
      "Completed 600 batches\n",
      "Completed 720 batches\n",
      "Completed 840 batches\n",
      "Completed 960 batches\n",
      "Completed 1080 batches\n",
      "Completed 1200 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 42 batches\n",
      "Completed 84 batches\n",
      "Completed 126 batches\n",
      "Completed 168 batches\n",
      "Completed 210 batches\n",
      "Epoch 20/50 : train loss 0.008 \t val loss 0.255\n",
      "Epoch 19 training done in 1112.609 seconds!\n",
      "Epoch: 20\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 120 batches\n",
      "Completed 240 batches\n",
      "Completed 360 batches\n",
      "Completed 480 batches\n",
      "Completed 600 batches\n",
      "Completed 720 batches\n",
      "Completed 840 batches\n",
      "Completed 960 batches\n",
      "Completed 1080 batches\n",
      "Completed 1200 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 42 batches\n",
      "Completed 84 batches\n",
      "Completed 126 batches\n",
      "Completed 168 batches\n",
      "Completed 210 batches\n",
      "New Best Model with loss 0.18532350659370422\n",
      "Epoch 21/50 : train loss 0.008 \t val loss 0.185\n",
      "Epoch 20 training done in 1117.786 seconds!\n",
      "Epoch: 21\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 120 batches\n",
      "Completed 240 batches\n",
      "Completed 360 batches\n",
      "Completed 480 batches\n",
      "Completed 600 batches\n",
      "Completed 720 batches\n",
      "Completed 840 batches\n",
      "Completed 960 batches\n",
      "Completed 1080 batches\n",
      "Completed 1200 batches\n",
      "Validation Started\n",
      "Completed 0 batches\n",
      "Completed 42 batches\n",
      "Completed 84 batches\n",
      "Completed 126 batches\n",
      "Completed 168 batches\n",
      "Completed 210 batches\n",
      "Epoch 22/50 : train loss 0.012 \t val loss 0.194\n",
      "Epoch 21 training done in 1140.074 seconds!\n",
      "Epoch: 22\n",
      "Training Started\n",
      "Completed 0 batches\n",
      "Completed 120 batches\n",
      "Completed 240 batches\n",
      "Completed 360 batches\n",
      "Completed 480 batches\n",
      "Completed 600 batches\n",
      "Completed 720 batches\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m### Training \u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Started\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m epoch_train_loss, batch_train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m### Validation  (use the testing function)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Started\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, device, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      7\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Iterate the dataloader (we do not need the label values, this is unsupervised learning and not supervised classification)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader): \u001b[38;5;66;03m# the variable `labels` will be used for customised training\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# reshape input\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(train_loader) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m batches\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataset.py:295\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mImageDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     12\u001b[0m label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 14\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, np\u001b[38;5;241m.\u001b[39msqueeze(label)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/albumentations/core/composition.py:205\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, force_apply, *args, **data)\u001b[0m\n\u001b[1;32m    202\u001b[0m     p\u001b[38;5;241m.\u001b[39mpreprocess(data)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(transforms):\n\u001b[0;32m--> 205\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_each_transform:\n\u001b[1;32m    208\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_data_post_transform(data)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/albumentations/core/transforms_interface.py:118\u001b[0m, in \u001b[0;36mBasicTransform.__call__\u001b[0;34m(self, force_apply, *args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m             warn(\n\u001b[1;32m    114\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_class_fullname() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m could work incorrectly in ReplayMode for other input data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m because its\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m params depend on targets.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m             )\n\u001b[1;32m    117\u001b[0m         kwargs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_key][\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)] \u001b[38;5;241m=\u001b[39m deepcopy(params)\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_with_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m kwargs\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/albumentations/core/transforms_interface.py:131\u001b[0m, in \u001b[0;36mBasicTransform.apply_with_params\u001b[0;34m(self, params, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m     target_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_target_function(key)\n\u001b[1;32m    130\u001b[0m     target_dependencies \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_dependence\u001b[38;5;241m.\u001b[39mget(key, [])}\n\u001b[0;32m--> 131\u001b[0m     res[key] \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtarget_dependencies\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     res[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/albumentations/augmentations/transforms.py:218\u001b[0m, in \u001b[0;36mNormalize.apply\u001b[0;34m(self, image, **params)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pixel_value\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/albumentations/augmentations/functional.py:96\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(img, mean, std, max_pixel_value)\u001b[0m\n\u001b[1;32m     93\u001b[0m denominator \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreciprocal(std, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m img\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnormalize_cv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenominator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m normalize_numpy(img, mean, denominator)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/albumentations/augmentations/functional.py:75\u001b[0m, in \u001b[0;36mnormalize_cv2\u001b[0;34m(img, mean, denominator)\u001b[0m\n\u001b[1;32m     73\u001b[0m img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(img\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     74\u001b[0m cv2\u001b[38;5;241m.\u001b[39msubtract(img, mean\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64), img)\n\u001b[0;32m---> 75\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenominator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Pytorch Native Train Loop\n",
    "\n",
    "train = True\n",
    "\n",
    "if train:\n",
    "    print('ViT training started')\n",
    "    history = {'train_loss':[],'val_loss':[], \"batch_train_loss\": []}\n",
    "    start_time = datetime.datetime.now()\n",
    "    folder_date = start_time.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "    model_ckpt_path = f\"ViT_model_train_{folder_date}\"\n",
    "\n",
    "    if not os.path.exists(f\"model_ckpts/{model_ckpt_path}\"):\n",
    "        os.mkdir(f\"model_ckpts/{model_ckpt_path}\")\n",
    "\n",
    "    best_val_loss = 10**10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch: {epoch}\")    \n",
    "        epoch_start_time = datetime.datetime.now()\n",
    "        ### Training \n",
    "        print(\"Training Started\")\n",
    "        epoch_train_loss, batch_train_loss = train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            device=device,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optim)\n",
    "        ### Validation  (use the testing function)\n",
    "        print(\"Validation Started\")\n",
    "        val_loss, predictions = test_model(\n",
    "            model=model,\n",
    "            test_loader=val_loader,\n",
    "            device=device,\n",
    "            loss_fn=loss_fn)\n",
    "        lr_scheduler.step()\n",
    "        # Print Losses \n",
    "        if (val_loss < best_val_loss):\n",
    "            print(f\"New Best Model with loss {val_loss}\")\n",
    "            torch.save(model.state_dict(), f\"model_ckpts/{model_ckpt_path}/{epoch}_model_loss_{val_loss}.ckpt\")\n",
    "            best_val_loss = val_loss\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} : train loss {epoch_train_loss:.3f} \\t val loss {val_loss:.3f}')\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['batch_train_loss'].extend(batch_train_loss)\n",
    "        with open(f\"model_ckpts/{model_ckpt_path}/epoch_{epoch}_model_results.pickle\", 'wb') as f:\n",
    "            pickle.dump(history, f)\n",
    "        print(f'Epoch {epoch} training done in {(datetime.datetime.now()-epoch_start_time).total_seconds():.3f} seconds!')\n",
    "\n",
    "\n",
    "\n",
    "    with open(f\"model_ckpts/{model_ckpt_path}/final_model_results.pickle\", 'wb') as f:\n",
    "        pickle.dump(history, f)\n",
    "\n",
    "    print(f'ViT training done in {(datetime.datetime.now()-start_time).total_seconds():.3f} seconds!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "504834f2-c82e-442d-a12d-467b29399594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(16,4))\n",
    "\n",
    "ax1.plot(history['train_loss'])\n",
    "ax1.title.set_text('Epoch Train Loss')\n",
    "\n",
    "ax2.plot(history['batch_train_loss'])\n",
    "ax2.title.set_text('Batch Train Loss')\n",
    "\n",
    "ax3.plot(history['val_loss'])\n",
    "ax3.title.set_text('Epoch Validation Loss')\n",
    "\n",
    "plt.savefig('vit_loss_graphs.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "276ab9ea-bb41-48f6-9b23-59b17d64bc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303 Samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-384 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([8, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 0 batches\n"
     ]
    }
   ],
   "source": [
    "# Test Model\n",
    "\n",
    "# test_df = pd.read_csv(\"project_data/train.csv\")\n",
    "\n",
    "# def get_frac_data_from_patient_test(target):\n",
    "#     patient_index = list(image_data.class_to_idx.values()).index(target)\n",
    "#     patient_name = list(image_data.class_to_idx.keys())[patient_index]\n",
    "#     row_for_pt = test_df[train_df[\"StudyInstanceUID\"] == patient_name]\n",
    "#     return row_for_pt.iloc[:,1:].values\n",
    "\n",
    "path = \"project_data/project_analysis/sag_test\"\n",
    "image_data_test = ImageFolder(root=path, target_transform=lambda x: np.array([1,1,0,0,0,0,0,0]))\n",
    "print(f\"{len(image_data_test)} Samples\")\n",
    "\n",
    "image_dataset_test = ImageDataset(image_data_test, base_augmentations)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(image_dataset_test, batch_size=batch_size,shuffle=False, num_workers=0)\n",
    "\n",
    "model = ViTForMultiClassPrediction()\n",
    "model.load_state_dict(torch.load('model_ckpts/ViT_model_train_2022_12_04_16_39_24/8_model_loss_0.19310231506824493.ckpt'))\n",
    "model.to(device)\n",
    "\n",
    "test_loss, predictions = test_model(\n",
    "    model=model,\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    loss_fn=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "68d9965e-45fc-4ca0-af20-bdafee8ae908",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_arr = predictions[0].numpy()\n",
    "actual_arr = predictions[1].numpy()\n",
    "thresholded_pred = np.where(pred_arr > 0.5, 1, 0)\n",
    "correct = (thresholded_pred == actual_arr).all(axis=1).nonzero()[0]\n",
    "\n",
    "for i,index in enumerate(correct):\n",
    "    im = image_data_test[index][0]\n",
    "    im.save(f'correct_vit_test_{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b767371-1d20-4003-ac1c-a74a3979c8f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
